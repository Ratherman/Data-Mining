{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f24d66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(lst): \n",
    "    return lst.index(max(lst))\n",
    "\n",
    "def find_max(col, DATA_COUNT):\n",
    "    max_num = -999\n",
    "    for i in range(DATA_COUNT):\n",
    "        if col[i] > max_num:\n",
    "            max_num = col[i]\n",
    "    return max_num\n",
    "\n",
    "def find_min_pos(col, DATA_COUNT, skip_list, k):\n",
    "    min_num = 100000\n",
    "    min_idx = 100000\n",
    "    \n",
    "    for num in range(DATA_COUNT):\n",
    "\n",
    "        toggle = False\n",
    "        for idx in range(k):\n",
    "            if num == skip_list[idx]:\n",
    "                toggle = True\n",
    "            \n",
    "        if col[num] < min_num and toggle == False:\n",
    "            min_num = col[num]\n",
    "            min_idx = num\n",
    "            \n",
    "    for num in range(k):\n",
    "        if skip_list[num] == -1:\n",
    "            skip_list[num] = min_idx\n",
    "            break\n",
    "    return min_idx, skip_list\n",
    "\n",
    "def find_max_pos(col, DATA_COUNT):\n",
    "    max_num = -999\n",
    "    pos = -999\n",
    "    for i in range(DATA_COUNT):\n",
    "        if col[i] > max_num:\n",
    "            max_num = col[i]\n",
    "            pos = i\n",
    "    return max_num, pos\n",
    "\n",
    "def find_min(col, DATA_COUNT):\n",
    "    min_num = 100000\n",
    "    for i in range(DATA_COUNT):\n",
    "        if col[i] < min_num:\n",
    "            min_num = col[i]\n",
    "    return min_num\n",
    "\n",
    "def sort_col(arr, DATA_COUNT):\n",
    "    \n",
    "    # 先讓 arr_sort 裡面全部都是 -100\n",
    "    arr_sort = []\n",
    "    arr_idx = []\n",
    "    \n",
    "    for i in range(DATA_COUNT):\n",
    "        arr_sort.append(-100)\n",
    "        arr_idx.append(-100)\n",
    "        \n",
    "    for count in range(DATA_COUNT):\n",
    "\n",
    "        # 找到當前 arr 裡面的最大值 以及 他的 index\n",
    "        max_number = -1000\n",
    "        max_number_idx = -1000\n",
    "        for i in range(DATA_COUNT):\n",
    "\n",
    "            if arr[i] > max_number:\n",
    "                max_number = arr[i]\n",
    "                max_number_idx = i\n",
    "\n",
    "        # 把他便最小，這樣之後就不會挑到他了\n",
    "        arr[max_number_idx] = -1000\n",
    "\n",
    "        # 把當前最大值存到 arr_sort 裡面\n",
    "        arr_sort[count] = max_number\n",
    "        arr_idx[count] = max_number_idx\n",
    "    return arr_sort, arr_idx\n",
    "\n",
    "def init_attributes(length):\n",
    "    init_attr_0, init_attr_1, init_attr_2, init_attr_3, init_attr_4, init_attr_5, init_attr_6, init_attr_7, init_attr_8 = [], [], [], [], [], [], [], [], []\n",
    "    for _ in range(length):\n",
    "        init_attr_0.append(-999)   \n",
    "        init_attr_1.append(-999)  \n",
    "        init_attr_2.append(-999)  \n",
    "        init_attr_3.append(-999)\n",
    "        init_attr_4.append(-999)\n",
    "        init_attr_5.append(-999)  \n",
    "        init_attr_6.append(-999)  \n",
    "        init_attr_7.append(-999)\n",
    "        init_attr_8.append(-999)\n",
    "\n",
    "    attributes = [init_attr_0, init_attr_1, init_attr_2, init_attr_3, init_attr_4, init_attr_5, init_attr_6, init_attr_7, init_attr_8]\n",
    "\n",
    "    return attributes\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def map_into_train_dataset(train_dataset, num_1, num_2, num_3, num_4, AF_1, AF_2, AF_3, AF_4):\n",
    "    \n",
    "    counter = 0\n",
    "    for idx in range(num_1):\n",
    "        for col_num in range(9):\n",
    "            train_dataset[col_num][counter] = AF_1[col_num][idx]\n",
    "        counter = counter + 1\n",
    "\n",
    "    for idx in range(num_2):\n",
    "        for col_num in range(9):\n",
    "            train_dataset[col_num][counter] = AF_2[col_num][idx]\n",
    "        counter = counter + 1\n",
    "\n",
    "    for idx in range(num_3):\n",
    "        for col_num in range(9):\n",
    "            train_dataset[col_num][counter] = AF_3[col_num][idx]\n",
    "        counter = counter + 1\n",
    "\n",
    "    for idx in range(num_4):\n",
    "        for col_num in range(9):\n",
    "            train_dataset[col_num][counter] = AF_4[col_num][idx]\n",
    "        counter = counter + 1\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "def naive_bayes_classifier(TSTA, test_num, TRNA, train_num):\n",
    "    ACC_Count = 0\n",
    "    for i in tqdm(range(test_num)):\n",
    "        \n",
    "        # Naive Bayes Classifier\n",
    "        count_c1 = 0\n",
    "        count_c2 = 0\n",
    "        for j in range(train_num):\n",
    "\n",
    "            # (1) 計算 p(c)    \n",
    "            if (TRNA[8][j] == 0): count_c1 = count_c1 + 1\n",
    "            if (TRNA[8][j] == 1): count_c2 = count_c2 + 1\n",
    "            \n",
    "        prob_c1 = count_c1 / train_num\n",
    "        prob_c2 = count_c2 / train_num\n",
    "\n",
    "        #for j in range(train_num):\n",
    "        \n",
    "        # (2) 計算 p(x,c)\n",
    "        cond_prob_c1=1\n",
    "        cond_prob_c2=1\n",
    "\n",
    "        for k in range(8):\n",
    "            target_count_c1 = 0\n",
    "            target_count_c2 = 0\n",
    "\n",
    "            for ii in range(train_num):\n",
    "\n",
    "                if   (TRNA[8][ii] == 0) and (TSTA[k][i] == TRNA[k][ii]): \n",
    "                    target_count_c1=target_count_c1+1\n",
    "\n",
    "                elif (TRNA[8][ii] == 1) and (TSTA[k][i] == TRNA[k][ii]):\n",
    "                    target_count_c2=target_count_c2+1\n",
    "\n",
    "            cond_prob_c1=cond_prob_c1*(target_count_c1+1)/count_c1\n",
    "            cond_prob_c2=cond_prob_c2*(target_count_c2+1)/count_c2\n",
    "\n",
    "        # (3) 比較 2 個 score 看誰最高\n",
    "        score_c1=prob_c1*cond_prob_c1\n",
    "        score_c2=prob_c2*cond_prob_c2\n",
    "\n",
    "        score_list = [score_c1, score_c2]\n",
    "\n",
    "        # (4) 看看最高的那個類別有沒有和 y_test 一致\n",
    "        max_score, max_position = find_max_pos(score_list, 2)\n",
    "\n",
    "        if   (TSTA[8][i] == 0) and (max_position == 0):\n",
    "            ACC_Count = ACC_Count + 1\n",
    "        elif (TSTA[8][i] == 1) and (max_position == 1):\n",
    "            ACC_Count = ACC_Count + 1\n",
    "        else:\n",
    "            ACC_Count = ACC_Count\n",
    "\n",
    "    return ACC_Count\n",
    "\n",
    "def KNN_classifier(TSTA, test_num, TRNA, train_num, mode):\n",
    "    ACC_Count = 0\n",
    "    \n",
    "    for i in tqdm(range(test_num)):\n",
    "        \n",
    "        # K Nearest Neighbor\n",
    "        test_instance = [TSTA[0][i], TSTA[1][i], TSTA[2][i], TSTA[3][i], TSTA[4][i], TSTA[5][i], TSTA[6][i], TSTA[7][i]]\n",
    "        # Create Diff List\n",
    "        diff_list = []\n",
    "        for idx in range(train_num):\n",
    "\n",
    "            diff_0 = (test_instance[0] - TRNA[0][idx])**2\n",
    "            diff_1 = (test_instance[1] - TRNA[1][idx])**2\n",
    "            diff_2 = (test_instance[2] - TRNA[2][idx])**2\n",
    "            diff_3 = (test_instance[3] - TRNA[3][idx])**2\n",
    "            diff_4 = (test_instance[4] - TRNA[4][idx])**2\n",
    "            diff_5 = (test_instance[5] - TRNA[5][idx])**2\n",
    "            diff_6 = (test_instance[6] - TRNA[6][idx])**2\n",
    "            diff_7 = (test_instance[7] - TRNA[7][idx])**2\n",
    "\n",
    "            total_diff = diff_0 + diff_1 + diff_2 + diff_3 + diff_4 + diff_5 + diff_6 + diff_7\n",
    "            diff_list.append(total_diff)\n",
    "        \n",
    "        # Find 5 min diff idx\n",
    "        skip_list=[-1, -1, -1, -1, -1]    \n",
    "        min_1_idx, skip_list = find_min_pos(diff_list, train_num, skip_list, 5)\n",
    "        min_2_idx, skip_list = find_min_pos(diff_list, train_num, skip_list, 5)\n",
    "        min_3_idx, skip_list = find_min_pos(diff_list, train_num, skip_list, 5)\n",
    "        min_4_idx, skip_list = find_min_pos(diff_list, train_num, skip_list, 5)\n",
    "        min_5_idx, skip_list = find_min_pos(diff_list, train_num, skip_list, 5)\n",
    "        \n",
    "        if mode == \"Majority_Voting\":\n",
    "\n",
    "            vote_1 = TRNA[8][min_1_idx]\n",
    "            vote_2 = TRNA[8][min_2_idx]\n",
    "            vote_3 = TRNA[8][min_3_idx]\n",
    "            vote_4 = TRNA[8][min_4_idx]\n",
    "            vote_5 = TRNA[8][min_5_idx]\n",
    "            \n",
    "            candidate_0 = 0\n",
    "            candidate_1 = 0\n",
    "            \n",
    "            if vote_1 == 0:\n",
    "                candidate_0 = candidate_0 + 1\n",
    "            else:\n",
    "                candidate_1 = candidate_1 + 1\n",
    "            \n",
    "            if vote_2 == 0:\n",
    "                candidate_0 = candidate_0 + 1\n",
    "            else:\n",
    "                candidate_1 = candidate_1 + 1\n",
    "                \n",
    "            if vote_3 == 0:\n",
    "                candidate_0 = candidate_0 + 1\n",
    "            else:\n",
    "                candidate_1 = candidate_1 + 1\n",
    "            \n",
    "            if vote_4 == 0:\n",
    "                candidate_0 = candidate_0 + 1\n",
    "            else:\n",
    "                candidate_1 = candidate_1 + 1\n",
    "            \n",
    "            if vote_5 == 0:\n",
    "                candidate_0 = candidate_0 + 1\n",
    "            else:\n",
    "                candidate_1 = candidate_1 + 1\n",
    "            \n",
    "            if candidate_0 > candidate_1:\n",
    "                pred_ans = 0\n",
    "            else:\n",
    "                pred_ans = 1\n",
    "            \n",
    "            if pred_ans == TSTA[8][i]:\n",
    "                ACC_Count = ACC_Count + 1\n",
    "            else:\n",
    "                ACC_Count = ACC_Count\n",
    "                \n",
    "        elif mode == \"Weighted_Voting\":\n",
    "            \n",
    "            vote_1 = TRNA[8][min_1_idx]\n",
    "            vote_2 = TRNA[8][min_2_idx]\n",
    "            vote_3 = TRNA[8][min_3_idx]\n",
    "            vote_4 = TRNA[8][min_4_idx]\n",
    "            vote_5 = TRNA[8][min_5_idx]\n",
    "            diff_1 = diff_list[min_1_idx]\n",
    "            diff_2 = diff_list[min_2_idx]\n",
    "            diff_3 = diff_list[min_3_idx]\n",
    "            diff_4 = diff_list[min_4_idx]\n",
    "            diff_5 = diff_list[min_5_idx]\n",
    "            candidate_0 = 0\n",
    "            candidate_1 = 0\n",
    "            \n",
    "            if vote_1 == 0:\n",
    "                candidate_0 = candidate_0 + 1/(diff_1**2)\n",
    "            else:\n",
    "                candidate_1 = candidate_1 + 1/(diff_1**2)\n",
    "            \n",
    "            if vote_2 == 0:\n",
    "                candidate_0 = candidate_0 + 1/(diff_2**2)\n",
    "            else:\n",
    "                candidate_1 = candidate_1 + 1/(diff_2**2)\n",
    "            \n",
    "            if vote_3 == 0:\n",
    "                candidate_0 = candidate_0 + 1/(diff_3**2)\n",
    "            else:\n",
    "                candidate_1 = candidate_1 + 1/(diff_3**2)\n",
    "            \n",
    "            if vote_4 == 0:\n",
    "                candidate_0 = candidate_0 + 1/(diff_4**2)\n",
    "            else:\n",
    "                candidate_1 = candidate_1 + 1/(diff_4**2)\n",
    "            \n",
    "            if vote_5 == 0:\n",
    "                candidate_0 = candidate_0 + 1/(diff_5**2)\n",
    "            else:\n",
    "                candidate_1 = candidate_1 + 1/(diff_5**2)\n",
    "            \n",
    "            if candidate_0 > candidate_1:\n",
    "                pred_ans = 0\n",
    "            else:\n",
    "                pred_ans = 1\n",
    "            \n",
    "            if pred_ans == TSTA[8][i]:\n",
    "                ACC_Count = ACC_Count + 1\n",
    "            else:\n",
    "                ACC_Count = ACC_Count\n",
    "                \n",
    "    return ACC_Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d6dd1",
   "metadata": {},
   "source": [
    "# Model1: Equal-width + NBC\n",
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "327cc958",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../pima.txt\")\n",
    "DATA = f.readlines()\n",
    "DATA_COUNT = len(DATA)\n",
    "f.close()\n",
    "\n",
    "# 觀察到 DATA 目前為 string 形式，透過 split 拆解 string，並存入 DATA_NEW\n",
    "# 另外，想要將 col 和 row 的位置對調\n",
    "\n",
    "attr_0, attr_1, attr_2, attr_3, attr_4, attr_5, attr_6, attr_7, attr_8 = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for row in DATA:\n",
    "\n",
    "    row_list = row.split(\",\")\n",
    "    \n",
    "    attr_0.append(float(row_list[0]))   \n",
    "    attr_1.append(float(row_list[1]))  \n",
    "    attr_2.append(float(row_list[2]))  \n",
    "    attr_3.append(float(row_list[3]))\n",
    "    attr_4.append(float(row_list[4]))\n",
    "    attr_5.append(float(row_list[5]))  \n",
    "    attr_6.append(float(row_list[6]))  \n",
    "    attr_7.append(float(row_list[7]))\n",
    "    attr_8.append(int(row_list[8]))\n",
    "\n",
    "# 我只要透過 Attributes[0] 就能取得 DATA 第一個 Attribute 的所有值（預計有768個）\n",
    "Attributes = [attr_0, attr_1, attr_2, attr_3, attr_4, attr_5, attr_6, attr_7, attr_8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c13d9e",
   "metadata": {},
   "source": [
    "# Model1: Equal-width + NBC\n",
    "\n",
    "## Equal-Width with 10 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5a590195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 0 個 Attribute\n",
      "第一區間 [15.3 ~ max_num]\n",
      "第二區間 [13.6 ~ 15.3)\n",
      "第三區間 [11.9 ~ 13.6)\n",
      "第四區間 [10.2 ~ 11.9)\n",
      "第五區間 [8.5 ~ 10.2)\n",
      "第六區間 [6.8 ~ 8.5)\n",
      "第七區間 [5.1 ~ 6.8)\n",
      "第八區間 [3.4 ~ 5.1)\n",
      "第九區間 [1.7 ~ 3.4)\n",
      "第十區間 [min_num ~ 1.7)\n",
      "\n",
      "第 1 個 Attribute\n",
      "第一區間 [179.1 ~ max_num]\n",
      "第二區間 [159.2 ~ 179.1)\n",
      "第三區間 [139.3 ~ 159.2)\n",
      "第四區間 [119.4 ~ 139.3)\n",
      "第五區間 [99.5 ~ 119.4)\n",
      "第六區間 [79.6 ~ 99.5)\n",
      "第七區間 [59.7 ~ 79.6)\n",
      "第八區間 [39.8 ~ 59.7)\n",
      "第九區間 [19.9 ~ 39.8)\n",
      "第十區間 [min_num ~ 19.9)\n",
      "\n",
      "第 2 個 Attribute\n",
      "第一區間 [109.8 ~ max_num]\n",
      "第二區間 [97.6 ~ 109.8)\n",
      "第三區間 [85.4 ~ 97.6)\n",
      "第四區間 [73.2 ~ 85.4)\n",
      "第五區間 [61.0 ~ 73.2)\n",
      "第六區間 [48.8 ~ 61.0)\n",
      "第七區間 [36.6 ~ 48.8)\n",
      "第八區間 [24.4 ~ 36.6)\n",
      "第九區間 [12.2 ~ 24.4)\n",
      "第十區間 [min_num ~ 12.2)\n",
      "\n",
      "第 3 個 Attribute\n",
      "第一區間 [89.1 ~ max_num]\n",
      "第二區間 [79.2 ~ 89.1)\n",
      "第三區間 [69.3 ~ 79.2)\n",
      "第四區間 [59.4 ~ 69.3)\n",
      "第五區間 [49.5 ~ 59.4)\n",
      "第六區間 [39.6 ~ 49.5)\n",
      "第七區間 [29.7 ~ 39.6)\n",
      "第八區間 [19.8 ~ 29.7)\n",
      "第九區間 [9.9 ~ 19.8)\n",
      "第十區間 [min_num ~ 9.9)\n",
      "\n",
      "第 4 個 Attribute\n",
      "第一區間 [761.4 ~ max_num]\n",
      "第二區間 [676.8 ~ 761.4)\n",
      "第三區間 [592.2 ~ 676.8)\n",
      "第四區間 [507.6 ~ 592.2)\n",
      "第五區間 [423.0 ~ 507.6)\n",
      "第六區間 [338.4 ~ 423.0)\n",
      "第七區間 [253.8 ~ 338.4)\n",
      "第八區間 [169.2 ~ 253.8)\n",
      "第九區間 [84.6 ~ 169.2)\n",
      "第十區間 [min_num ~ 84.6)\n",
      "\n",
      "第 5 個 Attribute\n",
      "第一區間 [60.39 ~ max_num]\n",
      "第二區間 [53.68 ~ 60.39)\n",
      "第三區間 [46.97 ~ 53.68)\n",
      "第四區間 [40.26 ~ 46.97)\n",
      "第五區間 [33.55 ~ 40.26)\n",
      "第六區間 [26.84 ~ 33.55)\n",
      "第七區間 [20.13 ~ 26.84)\n",
      "第八區間 [13.42 ~ 20.13)\n",
      "第九區間 [6.71 ~ 13.42)\n",
      "第十區間 [min_num ~ 6.71)\n",
      "\n",
      "第 6 個 Attribute\n",
      "第一區間 [2.186 ~ max_num]\n",
      "第二區間 [1.952 ~ 2.186)\n",
      "第三區間 [1.717 ~ 1.952)\n",
      "第四區間 [1.483 ~ 1.717)\n",
      "第五區間 [1.249 ~ 1.483)\n",
      "第六區間 [1.015 ~ 1.249)\n",
      "第七區間 [0.781 ~ 1.015)\n",
      "第八區間 [0.546 ~ 0.781)\n",
      "第九區間 [0.312 ~ 0.546)\n",
      "第十區間 [min_num ~ 0.312)\n",
      "\n",
      "第 7 個 Attribute\n",
      "第一區間 [75.0 ~ max_num]\n",
      "第二區間 [69.0 ~ 75.0)\n",
      "第三區間 [63.0 ~ 69.0)\n",
      "第四區間 [57.0 ~ 63.0)\n",
      "第五區間 [51.0 ~ 57.0)\n",
      "第六區間 [45.0 ~ 51.0)\n",
      "第七區間 [39.0 ~ 45.0)\n",
      "第八區間 [33.0 ~ 39.0)\n",
      "第九區間 [27.0 ~ 33.0)\n",
      "第十區間 [min_num ~ 27.0)\n"
     ]
    }
   ],
   "source": [
    "Attributes_EWD = []\n",
    "for i in range(9):\n",
    "    \n",
    "    specific_col = Attributes[i]\n",
    "    \n",
    "    if i != 8:\n",
    "        number_of_intervals = 10\n",
    "        max_num_in_col = find_max(specific_col, DATA_COUNT)\n",
    "        min_num_in_col = find_min(specific_col, DATA_COUNT)\n",
    "        width = (max_num_in_col - min_num_in_col)/number_of_intervals\n",
    "\n",
    "        # Decide 9 Split points, so that 10 intervals will form\n",
    "        split_point_1 = max_num_in_col - 1 * width\n",
    "        split_point_2 = max_num_in_col - 2 * width\n",
    "        split_point_3 = max_num_in_col - 3 * width\n",
    "        split_point_4 = max_num_in_col - 4 * width\n",
    "        split_point_5 = max_num_in_col - 5 * width\n",
    "        split_point_6 = max_num_in_col - 6 * width\n",
    "        split_point_7 = max_num_in_col - 7 * width\n",
    "        split_point_8 = max_num_in_col - 8 * width\n",
    "        split_point_9 = max_num_in_col - 9 * width\n",
    "\n",
    "        # transfer attributes into proper interval\n",
    "        discrete_attr = []\n",
    "        for j in range(DATA_COUNT):\n",
    "            discrete_attr.append(-100)\n",
    "\n",
    "        for j in range(DATA_COUNT):\n",
    "\n",
    "            num = specific_col[j]\n",
    "\n",
    "            if (num >= split_point_1): discrete_attr[j] = 0\n",
    "            elif (num >= split_point_2) and (num < split_point_1): discrete_attr[j] = 1\n",
    "            elif (num >= split_point_3) and (num < split_point_2): discrete_attr[j] = 2\n",
    "            elif (num >= split_point_4) and (num < split_point_3): discrete_attr[j] = 3\n",
    "            elif (num >= split_point_5) and (num < split_point_4): discrete_attr[j] = 4\n",
    "            elif (num >= split_point_6) and (num < split_point_5): discrete_attr[j] = 5\n",
    "            elif (num >= split_point_7) and (num < split_point_6): discrete_attr[j] = 6\n",
    "            elif (num >= split_point_8) and (num < split_point_7): discrete_attr[j] = 7\n",
    "            elif (num >= split_point_9) and (num < split_point_8): discrete_attr[j] = 8\n",
    "            elif (num  < split_point_9): discrete_attr[j] = 9\n",
    "                \n",
    "        Attributes_EWD.append(discrete_attr)\n",
    "        print(f\"\\n第 {str(i)} 個 Attribute\")\n",
    "        print(f\"第一區間 [{str(round(split_point_1,3))} ~ max_num]\")\n",
    "        print(f\"第二區間 [{str(round(split_point_2,3))} ~ {str(round(split_point_1,3))})\")\n",
    "        print(f\"第三區間 [{str(round(split_point_3,3))} ~ {str(round(split_point_2,3))})\")\n",
    "        print(f\"第四區間 [{str(round(split_point_4,3))} ~ {str(round(split_point_3,3))})\")\n",
    "        print(f\"第五區間 [{str(round(split_point_5,3))} ~ {str(round(split_point_4,3))})\")\n",
    "        print(f\"第六區間 [{str(round(split_point_6,3))} ~ {str(round(split_point_5,3))})\")\n",
    "        print(f\"第七區間 [{str(round(split_point_7,3))} ~ {str(round(split_point_6,3))})\")\n",
    "        print(f\"第八區間 [{str(round(split_point_8,3))} ~ {str(round(split_point_7,3))})\")\n",
    "        print(f\"第九區間 [{str(round(split_point_9,3))} ~ {str(round(split_point_8,3))})\")\n",
    "        print(f\"第十區間 [min_num ~ {str(round(split_point_9,3))})\")\n",
    "        \n",
    "    else:\n",
    "        Attributes_EWD.append(specific_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf5149",
   "metadata": {},
   "source": [
    "# Model1: Equal-width + NBC\n",
    "## Random Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "dfb9a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ================ #\n",
    "# get random index #\n",
    "# ================ #\n",
    "random_list = [] \n",
    "for _ in range(DATA_COUNT):\n",
    "    random_list.append(random.random())\n",
    "\n",
    "arr_sort, arr_ori_idx = sort_col(random_list, len(random_list))\n",
    "\n",
    "# ============================= #\n",
    "# Initialize Shuffle_Attributes #\n",
    "# ============================= #\n",
    "\n",
    "Shuffle_Attributes = init_attributes(DATA_COUNT)\n",
    "\n",
    "# ==================================== #\n",
    "# Put Attributes in Shuffle Attributes #\n",
    "# ==================================== #\n",
    "counter = 0\n",
    "for _ in range(DATA_COUNT):\n",
    "    ori_index = arr_ori_idx[counter]\n",
    "    for i in range(9):\n",
    "        Shuffle_Attributes[i][counter] = Attributes_EWD[i][ori_index]\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7fbb3",
   "metadata": {},
   "source": [
    "# Model1: Equal-width + NBC\n",
    "## Split into 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e3c49429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== #\n",
    "# Initialize 5 folds #\n",
    "# ================== #\n",
    "\n",
    "AF_1 = init_attributes(153)\n",
    "AF_2 = init_attributes(153)\n",
    "AF_3 = init_attributes(153)\n",
    "AF_4 = init_attributes(153)\n",
    "AF_5 = init_attributes(156)\n",
    "\n",
    "# ================== #\n",
    "# Split into 5 folds #\n",
    "# ================== #\n",
    "\n",
    "counter_k1 = 0\n",
    "counter_k2 = 0\n",
    "counter_k3 = 0\n",
    "counter_k4 = 0\n",
    "counter_k5 = 0\n",
    "\n",
    "for i in range(DATA_COUNT):\n",
    "    if i < 153*1:\n",
    "        for j in range(9):\n",
    "            AF_1[j][counter_k1] = Shuffle_Attributes[j][i]\n",
    "        counter_k1 = counter_k1 + 1\n",
    "        \n",
    "    elif i < 153*2 and i >= 153*1:\n",
    "        for j in range(9):\n",
    "            AF_2[j][counter_k2] = Shuffle_Attributes[j][i]\n",
    "        counter_k2 = counter_k2 + 1\n",
    "        \n",
    "    elif i < 153*3 and i >= 153*2:\n",
    "        for j in range(9):\n",
    "            AF_3[j][counter_k3] = Shuffle_Attributes[j][i]\n",
    "        counter_k3 = counter_k3 + 1\n",
    "\n",
    "    elif i < 153*4 and i >= 153*3:\n",
    "        for j in range(9):\n",
    "            AF_4[j][counter_k4] = Shuffle_Attributes[j][i]\n",
    "        counter_k4 = counter_k4 + 1\n",
    "        \n",
    "    elif i >= 153*4:\n",
    "        for j in range(9):\n",
    "            AF_5[j][counter_k5] = Shuffle_Attributes[j][i]\n",
    "        counter_k5 = counter_k5 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf10652",
   "metadata": {},
   "source": [
    "# Model1: Equal-width + NBC\n",
    "## Naive Baysian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "86cc848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 1172.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 1159.03it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 1159.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 1155.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 156/156 [00:00<00:00, 1189.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# ================================= #\n",
    "# Attributes_fold_1 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_1\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_2, AF_3, AF_4, AF_5)\n",
    "fold_1_correct = naive_bayes_classifier(test_dataset, 153, train_dataset, 615)\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_2 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_2\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_3, AF_4, AF_5)\n",
    "fold_2_correct = naive_bayes_classifier(test_dataset, 153, train_dataset, 615)\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_3 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_3\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_2, AF_4, AF_5)\n",
    "fold_3_correct = naive_bayes_classifier(test_dataset, 153, train_dataset, 615)\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_4 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_4\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_2, AF_3, AF_5)\n",
    "fold_4_correct = naive_bayes_classifier(test_dataset, 153, train_dataset, 615)\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_5 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_5\n",
    "train_dataset= init_attributes(153*4)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 153, AF_1, AF_2, AF_3, AF_4)\n",
    "fold_5_correct = naive_bayes_classifier(test_dataset, 156, train_dataset, 153*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062b0e6",
   "metadata": {},
   "source": [
    "# Model1: Equal-width + NBC\n",
    "## Calculate Average Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "bc47701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 121 118 108 119\n",
      "[Model 1: Equal-width Discretization + Naive Bayes Classifier] Average is 0.7630208333333334\n"
     ]
    }
   ],
   "source": [
    "print(fold_1_correct, fold_2_correct, fold_3_correct, fold_4_correct, fold_5_correct)\n",
    "P1_BAR = (fold_1_correct + fold_2_correct + fold_3_correct + fold_4_correct + fold_5_correct) / DATA_COUNT\n",
    "print(f\"[Model 1: Equal-width Discretization + Naive Bayes Classifier] Average is {P1_BAR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba3169c",
   "metadata": {},
   "source": [
    "# Model2: Equal-frequency + NBC\n",
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "caf7200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../pima.txt\")\n",
    "DATA = f.readlines()\n",
    "DATA_COUNT = len(DATA)\n",
    "f.close()\n",
    "\n",
    "# 觀察到 DATA 目前為 string 形式，透過 split 拆解 string，並存入 DATA_NEW\n",
    "# 另外，想要將 col 和 row 的位置對調\n",
    "\n",
    "attr_0, attr_1, attr_2, attr_3, attr_4, attr_5, attr_6, attr_7, attr_8 = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for row in DATA:\n",
    "\n",
    "    row_list = row.split(\",\")\n",
    "    \n",
    "    attr_0.append(float(row_list[0]))   \n",
    "    attr_1.append(float(row_list[1]))  \n",
    "    attr_2.append(float(row_list[2]))  \n",
    "    attr_3.append(float(row_list[3]))\n",
    "    attr_4.append(float(row_list[4]))\n",
    "    attr_5.append(float(row_list[5]))  \n",
    "    attr_6.append(float(row_list[6]))  \n",
    "    attr_7.append(float(row_list[7]))\n",
    "    attr_8.append(int(row_list[8]))\n",
    "\n",
    "# 我只要透過 Attributes[0] 就能取得 DATA 第一個 Attribute 的所有值（預計有768個）\n",
    "Attributes = [attr_0, attr_1, attr_2, attr_3, attr_4, attr_5, attr_6, attr_7, attr_8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af3fa0",
   "metadata": {},
   "source": [
    "# Model2: Equal-frequency + NBC\n",
    "## Equal-Frequency with 10 Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5955d9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 0 個 Attribute\n",
      "第一區間 [9.0 ~ max_num]\n",
      "第二區間 [7.0 ~ 9.0)\n",
      "第三區間 [5.0 ~ 7.0)\n",
      "第四區間 [4.0 ~ 5.0)\n",
      "第五區間 [3.0 ~ 4.0)\n",
      "第六區間 [2.0 ~ 3.0)\n",
      "第七區間 [1.0 ~ 2.0)\n",
      "第八區間 [1.0 ~ 1.0)\n",
      "第九區間 [0.0 ~ 1.0)\n",
      "第十區間 [min_num ~ 0.0)\n",
      "\n",
      "第 1 個 Attribute\n",
      "第一區間 [167.0 ~ max_num]\n",
      "第二區間 [147.0 ~ 167.0)\n",
      "第三區間 [135.0 ~ 147.0)\n",
      "第四區間 [125.0 ~ 135.0)\n",
      "第五區間 [117.0 ~ 125.0)\n",
      "第六區間 [109.0 ~ 117.0)\n",
      "第七區間 [102.0 ~ 109.0)\n",
      "第八區間 [95.0 ~ 102.0)\n",
      "第九區間 [87.0 ~ 95.0)\n",
      "第十區間 [min_num ~ 87.0)\n",
      "\n",
      "第 2 個 Attribute\n",
      "第一區間 [88.0 ~ max_num]\n",
      "第二區間 [82.0 ~ 88.0)\n",
      "第三區間 [78.0 ~ 82.0)\n",
      "第四區間 [74.0 ~ 78.0)\n",
      "第五區間 [72.0 ~ 74.0)\n",
      "第六區間 [68.0 ~ 72.0)\n",
      "第七區間 [64.0 ~ 68.0)\n",
      "第八區間 [62.0 ~ 64.0)\n",
      "第九區間 [54.0 ~ 62.0)\n",
      "第十區間 [min_num ~ 54.0)\n",
      "\n",
      "第 3 個 Attribute\n",
      "第一區間 [40.0 ~ max_num]\n",
      "第二區間 [35.0 ~ 40.0)\n",
      "第三區間 [31.0 ~ 35.0)\n",
      "第四區間 [27.0 ~ 31.0)\n",
      "第五區間 [23.0 ~ 27.0)\n",
      "第六區間 [18.0 ~ 23.0)\n",
      "第七區間 [10.0 ~ 18.0)\n",
      "第八區間 [0.0 ~ 10.0)\n",
      "第九區間 [0.0 ~ 0.0)\n",
      "第十區間 [min_num ~ 0.0)\n",
      "\n",
      "第 4 個 Attribute\n",
      "第一區間 [210.0 ~ max_num]\n",
      "第二區間 [152.0 ~ 210.0)\n",
      "第三區間 [108.0 ~ 152.0)\n",
      "第四區間 [74.0 ~ 108.0)\n",
      "第五區間 [36.0 ~ 74.0)\n",
      "第六區間 [0.0 ~ 36.0)\n",
      "第七區間 [0.0 ~ 0.0)\n",
      "第八區間 [0.0 ~ 0.0)\n",
      "第九區間 [0.0 ~ 0.0)\n",
      "第十區間 [min_num ~ 0.0)\n",
      "\n",
      "第 5 個 Attribute\n",
      "第一區間 [41.5 ~ max_num]\n",
      "第二區間 [37.8 ~ 41.5)\n",
      "第三區間 [35.5 ~ 37.8)\n",
      "第四區間 [33.8 ~ 35.5)\n",
      "第五區間 [32.2 ~ 33.8)\n",
      "第六區間 [30.1 ~ 32.2)\n",
      "第七區間 [28.4 ~ 30.1)\n",
      "第八區間 [26.1 ~ 28.4)\n",
      "第九區間 [23.9 ~ 26.1)\n",
      "第十區間 [min_num ~ 23.9)\n",
      "\n",
      "第 6 個 Attribute\n",
      "第一區間 [0.88 ~ max_num]\n",
      "第二區間 [0.692 ~ 0.88)\n",
      "第三區間 [0.569 ~ 0.692)\n",
      "第四區間 [0.457 ~ 0.569)\n",
      "第五區間 [0.378 ~ 0.457)\n",
      "第六區間 [0.304 ~ 0.378)\n",
      "第七區間 [0.26 ~ 0.304)\n",
      "第八區間 [0.225 ~ 0.26)\n",
      "第九區間 [0.167 ~ 0.225)\n",
      "第十區間 [min_num ~ 0.167)\n",
      "\n",
      "第 7 個 Attribute\n",
      "第一區間 [51.0 ~ max_num]\n",
      "第二區間 [43.0 ~ 51.0)\n",
      "第三區間 [38.0 ~ 43.0)\n",
      "第四區間 [33.0 ~ 38.0)\n",
      "第五區間 [29.0 ~ 33.0)\n",
      "第六區間 [27.0 ~ 29.0)\n",
      "第七區間 [25.0 ~ 27.0)\n",
      "第八區間 [23.0 ~ 25.0)\n",
      "第九區間 [22.0 ~ 23.0)\n",
      "第十區間 [min_num ~ 22.0)\n"
     ]
    }
   ],
   "source": [
    "Attributes_Sort = []\n",
    "\n",
    "for i in range(9):\n",
    "    if i < 8:\n",
    "        col, _ = sort_col(Attributes[i], DATA_COUNT)\n",
    "        Attributes_Sort.append(col)\n",
    "    else:\n",
    "        Attributes_Sort.append(Attributes[i])\n",
    "\n",
    "##############\n",
    "# Read Again #\n",
    "##############\n",
    "f = open(\"../pima.txt\")\n",
    "DATA = f.readlines()\n",
    "DATA_COUNT = len(DATA)\n",
    "f.close()\n",
    "\n",
    "number_of_intervals = 10\n",
    "number_of_attr_in_interval = DATA_COUNT / number_of_intervals\n",
    "\n",
    "attr_0, attr_1, attr_2, attr_3, attr_4, attr_5, attr_6, attr_7, attr_8 = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for row in DATA:\n",
    "\n",
    "    row_list = row.split(\",\")\n",
    "    \n",
    "    attr_0.append(float(row_list[0]))   \n",
    "    attr_1.append(float(row_list[1]))  \n",
    "    attr_2.append(float(row_list[2]))  \n",
    "    attr_3.append(float(row_list[3]))\n",
    "    attr_4.append(float(row_list[4]))\n",
    "    attr_5.append(float(row_list[5]))  \n",
    "    attr_6.append(float(row_list[6]))  \n",
    "    attr_7.append(float(row_list[7]))\n",
    "    attr_8.append(int(row_list[8]))\n",
    "    \n",
    "\n",
    "# 我只要透過 Attributes[0] 就能取得 DATA 第一個 Attribute 的所有值（預計有768個）\n",
    "Attributes = [attr_0, attr_1, attr_2, attr_3, attr_4, attr_5, attr_6, attr_7, attr_8]\n",
    "\n",
    "\n",
    "Attributes_EFD = []\n",
    "\n",
    "for j in range(9):\n",
    "    specific_col_sort = Attributes_Sort[j]\n",
    "    specific_col = Attributes[j]\n",
    "        \n",
    "    if j != 8:\n",
    "\n",
    "        split_point_1 = specific_col_sort[1 * int(number_of_attr_in_interval)]\n",
    "        split_point_2 = specific_col_sort[2 * int(number_of_attr_in_interval)]\n",
    "        split_point_3 = specific_col_sort[3 * int(number_of_attr_in_interval)]\n",
    "        split_point_4 = specific_col_sort[4 * int(number_of_attr_in_interval)]\n",
    "        split_point_5 = specific_col_sort[5 * int(number_of_attr_in_interval)]\n",
    "        split_point_6 = specific_col_sort[6 * int(number_of_attr_in_interval)]\n",
    "        split_point_7 = specific_col_sort[7 * int(number_of_attr_in_interval)]\n",
    "        split_point_8 = specific_col_sort[8 * int(number_of_attr_in_interval)]\n",
    "        split_point_9 = specific_col_sort[9 * int(number_of_attr_in_interval)]\n",
    "\n",
    "        # transfer attributes into proper interval\n",
    "        discrete_attr = []\n",
    "        for i in range(DATA_COUNT):\n",
    "            discrete_attr.append(-100)\n",
    "\n",
    "        for i in range(DATA_COUNT):\n",
    "\n",
    "            num = specific_col[i]\n",
    "\n",
    "            if num >= split_point_1: discrete_attr[i] = 0\n",
    "            elif num >= split_point_2 and num < split_point_1: discrete_attr[i] = 1\n",
    "            elif num >= split_point_3 and num < split_point_2: discrete_attr[i] = 2\n",
    "            elif num >= split_point_4 and num < split_point_3: discrete_attr[i] = 3\n",
    "            elif num >= split_point_5 and num < split_point_4: discrete_attr[i] = 4\n",
    "            elif num >= split_point_6 and num < split_point_5: discrete_attr[i] = 5\n",
    "            elif num >= split_point_7 and num < split_point_6: discrete_attr[i] = 6\n",
    "            elif num >= split_point_8 and num < split_point_7: discrete_attr[i] = 7\n",
    "            elif num >= split_point_9 and num < split_point_8: discrete_attr[i] = 8\n",
    "            elif num < split_point_9: discrete_attr[i] = 9\n",
    "                \n",
    "        Attributes_EFD.append(discrete_attr)\n",
    "        print(f\"\\n第 {str(j)} 個 Attribute\")\n",
    "        print(f\"第一區間 [{str(round(split_point_1,3))} ~ max_num]\")\n",
    "        print(f\"第二區間 [{str(round(split_point_2,3))} ~ {str(round(split_point_1,3))})\")\n",
    "        print(f\"第三區間 [{str(round(split_point_3,3))} ~ {str(round(split_point_2,3))})\")\n",
    "        print(f\"第四區間 [{str(round(split_point_4,3))} ~ {str(round(split_point_3,3))})\")\n",
    "        print(f\"第五區間 [{str(round(split_point_5,3))} ~ {str(round(split_point_4,3))})\")\n",
    "        print(f\"第六區間 [{str(round(split_point_6,3))} ~ {str(round(split_point_5,3))})\")\n",
    "        print(f\"第七區間 [{str(round(split_point_7,3))} ~ {str(round(split_point_6,3))})\")\n",
    "        print(f\"第八區間 [{str(round(split_point_8,3))} ~ {str(round(split_point_7,3))})\")\n",
    "        print(f\"第九區間 [{str(round(split_point_9,3))} ~ {str(round(split_point_8,3))})\")\n",
    "        print(f\"第十區間 [min_num ~ {str(round(split_point_9,3))})\")\n",
    "    else:\n",
    "        Attributes_EFD.append(specific_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558eab5",
   "metadata": {},
   "source": [
    "# Model2: Equal-frequency + NBC\n",
    "## Random Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "007d4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ================ #\n",
    "# get random index #\n",
    "# ================ #\n",
    "random_list = [] \n",
    "for _ in range(DATA_COUNT):\n",
    "    random_list.append(random.random())\n",
    "\n",
    "arr_sort, arr_ori_idx = sort_col(random_list, len(random_list))\n",
    "\n",
    "# ============================= #\n",
    "# Initialize Shuffle_Attributes #\n",
    "# ============================= #\n",
    "\n",
    "Shuffle_Attributes = init_attributes(DATA_COUNT)\n",
    "\n",
    "# ==================================== #\n",
    "# Put Attributes in Shuffle Attributes #\n",
    "# ==================================== #\n",
    "counter = 0\n",
    "for _ in range(DATA_COUNT):\n",
    "    ori_index = arr_ori_idx[counter]\n",
    "    for i in range(9):\n",
    "        Shuffle_Attributes[i][counter] = Attributes_EFD[i][ori_index]\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c4a5f",
   "metadata": {},
   "source": [
    "# Model2: Equal-Frequency + NBC\n",
    "## Split into 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a5642d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== #\n",
    "# Initialize 5 folds #\n",
    "# ================== #\n",
    "\n",
    "AF_1 = init_attributes(153)\n",
    "AF_2 = init_attributes(153)\n",
    "AF_3 = init_attributes(153)\n",
    "AF_4 = init_attributes(153)\n",
    "AF_5 = init_attributes(156)\n",
    "\n",
    "# ================== #\n",
    "# Split into 5 folds #\n",
    "# ================== #\n",
    "\n",
    "counter_k1 = 0\n",
    "counter_k2 = 0\n",
    "counter_k3 = 0\n",
    "counter_k4 = 0\n",
    "counter_k5 = 0\n",
    "\n",
    "for i in range(DATA_COUNT):\n",
    "    if i < 153*1:\n",
    "        for j in range(9):\n",
    "            AF_1[j][counter_k1] = Shuffle_Attributes[j][i]\n",
    "        counter_k1 = counter_k1 + 1\n",
    "        \n",
    "    elif i < 153*2 and i >= 153*1:\n",
    "        for j in range(9):\n",
    "            AF_2[j][counter_k2] = Shuffle_Attributes[j][i]\n",
    "        counter_k2 = counter_k2 + 1\n",
    "        \n",
    "    elif i < 153*3 and i >= 153*2:\n",
    "        for j in range(9):\n",
    "            AF_3[j][counter_k3] = Shuffle_Attributes[j][i]\n",
    "        counter_k3 = counter_k3 + 1\n",
    "\n",
    "    elif i < 153*4 and i >= 153*3:\n",
    "        for j in range(9):\n",
    "            AF_4[j][counter_k4] = Shuffle_Attributes[j][i]\n",
    "        counter_k4 = counter_k4 + 1\n",
    "        \n",
    "    elif i >= 153*4:\n",
    "        for j in range(9):\n",
    "            AF_5[j][counter_k5] = Shuffle_Attributes[j][i]\n",
    "        counter_k5 = counter_k5 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a490f37",
   "metadata": {},
   "source": [
    "# Model2: Equal-Frequency + NBC\n",
    "## Naive Baysian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "177d82eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 1189.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 1254.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 1172.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 1229.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 156/156 [00:00<00:00, 1234.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# ================================= #\n",
    "# Attributes_fold_1 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_1\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_2, AF_3, AF_4, AF_5)\n",
    "fold_1_correct = naive_bayes_classifier(test_dataset, 153, train_dataset, 615)\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_2 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_2\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_3, AF_4, AF_5)\n",
    "fold_2_correct = naive_bayes_classifier(test_dataset, 153, train_dataset, 615)\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_3 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_3\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_2, AF_4, AF_5)\n",
    "fold_3_correct = naive_bayes_classifier(test_dataset, 153, train_dataset, 615)\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_4 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_4\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_2, AF_3, AF_5)\n",
    "fold_4_correct = naive_bayes_classifier(test_dataset, 153, train_dataset, 615)\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_5 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_5\n",
    "train_dataset= init_attributes(153*4)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 153, AF_1, AF_2, AF_3, AF_4)\n",
    "fold_5_correct = naive_bayes_classifier(test_dataset, 156, train_dataset, 153*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a2380234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 113 109 117 119\n",
      "[Model 2: Equal-frequency Discretization + Naive Bayes Classifier] Average is 0.75\n"
     ]
    }
   ],
   "source": [
    "print(fold_1_correct, fold_2_correct, fold_3_correct, fold_4_correct, fold_5_correct)\n",
    "\n",
    "P2_BAR = (fold_1_correct + fold_2_correct + fold_3_correct + fold_4_correct + fold_5_correct) / DATA_COUNT\n",
    "print(f\"[Model 2: Equal-frequency Discretization + Naive Bayes Classifier] Average is {P2_BAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4b38ed7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Model 1 and Model 2.\n",
      "H0: P1_BAR = P2_BAR\n",
      "(Two-tailed test with alpha = 5%) Z = 0.5945066853600948\n",
      "H0 is NOT rejected.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(\"Compare Model 1 and Model 2.\")\n",
    "print(\"H0: P1_BAR = P2_BAR\")\n",
    "\n",
    "P_AVG = (P1_BAR + P2_BAR) / 2\n",
    "\n",
    "Z = (P1_BAR-P2_BAR)/math.sqrt(2/DATA_COUNT*P_AVG*(1-P_AVG))\n",
    "\n",
    "print(f\"(Two-tailed test with alpha = 5%) Z = {Z}\")\n",
    "if Z < -1.96 or Z > 1.96:\n",
    "    print(\"H0 is rejected.\")\n",
    "else:\n",
    "    print(\"H0 is NOT rejected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19881baf",
   "metadata": {},
   "source": [
    "# Model 3: KNN + Majority Voting\n",
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1a08df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../pima.txt\")\n",
    "DATA = f.readlines()\n",
    "DATA_COUNT = len(DATA)\n",
    "f.close()\n",
    "\n",
    "# 觀察到 DATA 目前為 string 形式，透過 split 拆解 string，並存入 DATA_NEW\n",
    "# 另外，想要將 col 和 row 的位置對調\n",
    "\n",
    "attr_0, attr_1, attr_2, attr_3, attr_4, attr_5, attr_6, attr_7, attr_8 = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for row in DATA:\n",
    "\n",
    "    row_list = row.split(\",\")\n",
    "    \n",
    "    attr_0.append(float(row_list[0]))\n",
    "    attr_1.append(float(row_list[1]))  \n",
    "    attr_2.append(float(row_list[2]))  \n",
    "    attr_3.append(float(row_list[3]))\n",
    "    attr_4.append(float(row_list[4]))\n",
    "    attr_5.append(float(row_list[5]))  \n",
    "    attr_6.append(float(row_list[6]))  \n",
    "    attr_7.append(float(row_list[7]))\n",
    "    attr_8.append(int(row_list[8]))\n",
    "\n",
    "# 我只要透過 Attributes[0] 就能取得 DATA 第一個 Attribute 的所有值（預計有768個）\n",
    "Attributes = [attr_0, attr_1, attr_2, attr_3, attr_4, attr_5, attr_6, attr_7, attr_8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d787eed",
   "metadata": {},
   "source": [
    "# Model 3: KNN + Majority Voting\n",
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "86020e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(DATA_COUNT):\n",
    "    Attributes_Norm = []\n",
    "    for col_num in range(8):\n",
    "        max_attr = find_max(Attributes[col_num], DATA_COUNT)\n",
    "        min_attr = find_min(Attributes[col_num], DATA_COUNT)\n",
    "        attr_norm = []\n",
    "        for num in range(DATA_COUNT):\n",
    "            attr_norm.append((Attributes[col_num][num]-min_attr)/(max_attr-min_attr))\n",
    "        Attributes_Norm.append(attr_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a32af",
   "metadata": {},
   "source": [
    "# Model 3: KNN + Majority Voting\n",
    "## Random Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6700e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ================ #\n",
    "# get random index #\n",
    "# ================ #\n",
    "random_list = [] \n",
    "for _ in range(DATA_COUNT):\n",
    "    random_list.append(random.random())\n",
    "\n",
    "arr_sort, arr_ori_idx = sort_col(random_list, len(random_list))\n",
    "\n",
    "# ============================= #\n",
    "# Initialize Shuffle_Attributes #\n",
    "# ============================= #\n",
    "\n",
    "Shuffle_Attributes = init_attributes(DATA_COUNT)\n",
    "\n",
    "# ==================================== #\n",
    "# Put Attributes in Shuffle Attributes #\n",
    "# ==================================== #\n",
    "counter = 0\n",
    "for _ in range(DATA_COUNT):\n",
    "    ori_index = arr_ori_idx[counter]\n",
    "    for i in range(8):\n",
    "        Shuffle_Attributes[i][counter] = Attributes_Norm[i][ori_index]\n",
    "    Shuffle_Attributes[8][counter] = Attributes[8][ori_index]\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585d087",
   "metadata": {},
   "source": [
    "# Model 3: KNN + Majority Voting\n",
    "## Split into 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "526c2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== #\n",
    "# Initialize 5 folds #\n",
    "# ================== #\n",
    "\n",
    "AF_1 = init_attributes(153)\n",
    "AF_2 = init_attributes(153)\n",
    "AF_3 = init_attributes(153)\n",
    "AF_4 = init_attributes(153)\n",
    "AF_5 = init_attributes(156)\n",
    "\n",
    "# ================== #\n",
    "# Split into 5 folds #\n",
    "# ================== #\n",
    "\n",
    "counter_k1 = 0\n",
    "counter_k2 = 0\n",
    "counter_k3 = 0\n",
    "counter_k4 = 0\n",
    "counter_k5 = 0\n",
    "\n",
    "for i in range(DATA_COUNT):\n",
    "    if i < 153*1:\n",
    "        for j in range(9):\n",
    "            AF_1[j][counter_k1] = Shuffle_Attributes[j][i]\n",
    "        counter_k1 = counter_k1 + 1\n",
    "        \n",
    "    elif i < 153*2 and i >= 153*1:\n",
    "        for j in range(9):\n",
    "            AF_2[j][counter_k2] = Shuffle_Attributes[j][i]\n",
    "        counter_k2 = counter_k2 + 1\n",
    "        \n",
    "    elif i < 153*3 and i >= 153*2:\n",
    "        for j in range(9):\n",
    "            AF_3[j][counter_k3] = Shuffle_Attributes[j][i]\n",
    "        counter_k3 = counter_k3 + 1\n",
    "\n",
    "    elif i < 153*4 and i >= 153*3:\n",
    "        for j in range(9):\n",
    "            AF_4[j][counter_k4] = Shuffle_Attributes[j][i]\n",
    "        counter_k4 = counter_k4 + 1\n",
    "        \n",
    "    elif i >= 153*4:\n",
    "        for j in range(9):\n",
    "            AF_5[j][counter_k5] = Shuffle_Attributes[j][i]\n",
    "        counter_k5 = counter_k5 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5376633",
   "metadata": {},
   "source": [
    "# Model 3: KNN + Majority Voting\n",
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c33e9ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 482.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 511.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 504.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 515.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 156/156 [00:00<00:00, 516.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# ================================= #\n",
    "# Attributes_fold_1 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_1\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_2, AF_3, AF_4, AF_5)\n",
    "fold_1_correct = KNN_classifier(test_dataset, 153, train_dataset, 615, mode=\"Majority_Voting\")\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_2 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_2\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_3, AF_4, AF_5)\n",
    "fold_2_correct = KNN_classifier(test_dataset, 153, train_dataset, 615, mode=\"Majority_Voting\")\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_3 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_3\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_2, AF_4, AF_5)\n",
    "fold_3_correct = KNN_classifier(test_dataset, 153, train_dataset, 615, mode=\"Majority_Voting\")\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_4 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_4\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_2, AF_3, AF_5)\n",
    "fold_4_correct = KNN_classifier(test_dataset, 153, train_dataset, 615, mode=\"Majority_Voting\")\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_5 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_5\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 153, AF_1, AF_2, AF_3, AF_4)\n",
    "fold_5_correct = KNN_classifier(test_dataset, 156, train_dataset, 153*4, mode=\"Majority_Voting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "89d31737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 111 124 110 121\n",
      "[Model 3: K-Nearest Neighbor + Majority Voting] Average is 0.7434895833333334\n"
     ]
    }
   ],
   "source": [
    "print(fold_1_correct, fold_2_correct, fold_3_correct, fold_4_correct, fold_5_correct)\n",
    "\n",
    "P3_BAR = (fold_1_correct + fold_2_correct + fold_3_correct + fold_4_correct + fold_5_correct) / DATA_COUNT\n",
    "print(f\"[Model 3: K-Nearest Neighbor + Majority Voting] Average is {P3_BAR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a35e2c",
   "metadata": {},
   "source": [
    "# Model 4: KNN + Distance Weighted Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c209631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 511.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 515.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 520.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 501.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 156/156 [00:00<00:00, 504.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# ================================= #\n",
    "# Attributes_fold_1 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_1\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_2, AF_3, AF_4, AF_5)\n",
    "fold_1_correct = KNN_classifier(test_dataset, 153, train_dataset, 615, mode=\"Weighted_Voting\")\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_2 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_2\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_3, AF_4, AF_5)\n",
    "fold_2_correct = KNN_classifier(test_dataset, 153, train_dataset, 615, mode=\"Weighted_Voting\")\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_3 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_3\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_2, AF_4, AF_5)\n",
    "fold_3_correct = KNN_classifier(test_dataset, 153, train_dataset, 615, mode=\"Weighted_Voting\")\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_4 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_4\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 156, AF_1, AF_2, AF_3, AF_5)\n",
    "fold_4_correct = KNN_classifier(test_dataset, 153, train_dataset, 615, mode=\"Weighted_Voting\")\n",
    "\n",
    "# ================================= #\n",
    "# Attributes_fold_5 當 test dataset #\n",
    "# ================================= #\n",
    "test_dataset = AF_5\n",
    "train_dataset= init_attributes(153*3+156)\n",
    "train_dataset= map_into_train_dataset(train_dataset, 153, 153, 153, 153, AF_1, AF_2, AF_3, AF_4)\n",
    "fold_5_correct = KNN_classifier(test_dataset, 156, train_dataset, 153*4, mode=\"Weighted_Voting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "65ce1a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 109 121 112 120\n",
      "[Model 4: K-Nearest Neighbor + Distance-Weighted Voting] Average is 0.7356770833333334\n"
     ]
    }
   ],
   "source": [
    "print(fold_1_correct, fold_2_correct, fold_3_correct, fold_4_correct, fold_5_correct)\n",
    "\n",
    "P4_BAR = (fold_1_correct + fold_2_correct + fold_3_correct + fold_4_correct + fold_5_correct) / DATA_COUNT\n",
    "print(f\"[Model 4: K-Nearest Neighbor + Distance-Weighted Voting] Average is {P4_BAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d57bc0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Model 3 and Model 4.\n",
      "H0: P3_BAR = P4_BAR\n",
      "(Two-tailed test with alpha = 5%) Z = 0.34884113984028664\n",
      "H0 is NOT rejected.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(\"Compare Model 3 and Model 4.\")\n",
    "print(\"H0: P3_BAR = P4_BAR\")\n",
    "\n",
    "P_AVG = (P3_BAR + P4_BAR) / 2\n",
    "\n",
    "Z = (P3_BAR-P4_BAR)/math.sqrt(2/DATA_COUNT*P_AVG*(1-P_AVG))\n",
    "\n",
    "print(f\"(Two-tailed test with alpha = 5%) Z = {Z}\")\n",
    "if Z < -1.96 or Z > 1.96:\n",
    "    print(\"H0 is rejected.\")\n",
    "else:\n",
    "    print(\"H0 is NOT rejected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "81b50f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Model 2 and Model 3.\n",
      "H0: P2_BAR = P3_BAR\n",
      "(Two-tailed test with alpha = 5%) Z = 0.2933655428352383\n",
      "H0 is NOT rejected.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(\"Compare Model 2 and Model 3.\")\n",
    "print(\"H0: P2_BAR = P3_BAR\")\n",
    "\n",
    "P_AVG = (P2_BAR + P3_BAR) / 2\n",
    "\n",
    "Z = (P2_BAR-P3_BAR)/math.sqrt(2/DATA_COUNT*P_AVG*(1-P_AVG))\n",
    "\n",
    "print(f\"(Two-tailed test with alpha = 5%) Z = {Z}\")\n",
    "if Z < -1.96 or Z > 1.96:\n",
    "    print(\"H0 is rejected.\")\n",
    "else:\n",
    "    print(\"H0 is NOT rejected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f68d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
